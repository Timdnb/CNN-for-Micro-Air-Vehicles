{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for CNN training for AE4317 Autonomous Flight of Micro Air Vehicles\n",
    "This notebook contains everything needed to train a Convolutional Neural Network to be deployed on a drone. It allows maximum flexibility, giving you the opportunity the find the best architecture. To be able to keep track of all you models and see how they are performing, it is recommended to use [Weights and Biases](https://wandb.ai/), as this is already set up in the logging.\n",
    "\n",
    "By now you should have a labeled dataset either generated using the previous notebook `dataset_generation.ipynb` or of your own.\n",
    "\n",
    "Make sure to go through the entire notebook at least once, and try to understand what every part does. In the end you will mostly be using the parameter dashboard, as this where you change the model architecture and hyperparameters.\n",
    "\n",
    "Uses torch lightning, link blabla\n",
    "\n",
    "Some explanation on onnx to 2c + limitations that it brings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Below are all the imports we need, make sure to have all of them installed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calc_mean_std_dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "import torchsummary\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Dashboard\n",
    "This is your main place to tune your CNN. You can fully specify the architecture and a few other hyperparameters, such as the number of epochs and learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "label_smoothing = 0.0 # prob 0\n",
    "normalize_images = True\n",
    "batch_norm = True\n",
    "\n",
    "analyze_model_before_training = False\n",
    "\n",
    "# CNN Architecture\n",
    "architecture = {\n",
    "    \"n_layers\": 4,\n",
    "    \"conv_layers\": {\n",
    "        \"input_size\": (1, 65, 15),\n",
    "        \"output_channel\": (8, 16, 32, 64),\n",
    "        \"kernel_size\": (3, 3, 3, 3),\n",
    "        \"stride\": (1, 1, 1, 1),\n",
    "        \"padding\": (1, 1, 1, 1),   \n",
    "    },\n",
    "    \"batch_norm\": batch_norm,\n",
    "    \"max_pool_layers\": {\n",
    "        \"kernel_size\": ((2,1), 2, 2, 2),\n",
    "        \"stride\": ((2,1), 2, 2, 2)\n",
    "    },\n",
    "    \"dropout_layers\": {\n",
    "        \"p\": (0.1, 0.075, 0.05, 0.025)\n",
    "    },\n",
    "    \"fc_layer\": {\n",
    "        \"input_size\": None,\n",
    "        \"output_size\": 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "This dataset assumes that you have a .csv with the first column being the file name of the image and the other columns that label. If this does not apply to your dataset, make sure to update it accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class DroneImagesDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, skiprows=1, header=None)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.annotations.iloc[index, 0]\n",
    "        # img_path = str(Path(img_path)) # TODOOOOO\n",
    "        img_path = img_path.replace(\"\\\\\", \"/\")\n",
    "        image = convert_image_dtype(read_image(img_path), torch.float)\n",
    "        y_label = torch.tensor(list(self.annotations.iloc[index, 1:]), dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, y_label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two transforms, one for training, which is used for data augmentation and one which is used during validation and testing. Using the training data transform to augment images can be rather slow, so you might one to use that once you have a proper model and not while quickly prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to try: bilinear, bicubic or nearest exact\n",
    "IMAGE_TRANSFORM = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.CenterCrop((520, 120)),\n",
    "    transforms.Resize((architecture[\"conv_layers\"][\"input_size\"][1], architecture[\"conv_layers\"][\"input_size\"][2]), interpolation=transforms.InterpolationMode.NEAREST_EXACT),\n",
    "])\n",
    "\n",
    "TRAIN_IMAGE_TRANSFORM = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.3, hue=0.05),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.CenterCrop((520, 120)),\n",
    "    transforms.Resize((architecture[\"conv_layers\"][\"input_size\"][1], architecture[\"conv_layers\"][\"input_size\"][2]), interpolation=transforms.InterpolationMode.NEAREST_EXACT),\n",
    "])\n",
    "\n",
    "if normalize_images:\n",
    "    # mean, std = calc_mean_std_dataset('all_images', IMAGE_TRANSFORM)\n",
    "    mean, std = 0.30496492981910706, 0.1657978892326355 # (1, 65, 15)\n",
    "    # mean, std = 0.3050636351108551, 0.16549718379974365 # (1, 26, 12)\n",
    "    # mean, std = 0.3046516478061676, 0.1654612421989441 # (1, 40, 12)\n",
    "    print(f\"Mean: {mean}, Std: {std}\")\n",
    "\n",
    "    IMAGE_TRANSFORM = transforms.Compose([IMAGE_TRANSFORM,\n",
    "        transforms.Normalize(mean=[mean], std=[std])\n",
    "    ])\n",
    "\n",
    "    TRAIN_IMAGE_TRANSFORM = transforms.Compose([TRAIN_IMAGE_TRANSFORM,\n",
    "        transforms.Normalize(mean=[mean], std=[std])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create dataloaders for our training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "batch_size = 128\n",
    "dataset = DroneImagesDataset(csv_file='labeled_images.csv', transform=IMAGE_TRANSFORM) # TRY AND OVERFIT ??\n",
    "\n",
    "# # count how many per class --------------> [5950. 6670. 5950.]\n",
    "# labels = []\n",
    "# for i in range(len(dataset)):\n",
    "#     labels.append(dataset[i][1].tolist())\n",
    "# labels = np.array(labels)\n",
    "# print(np.sum(labels, axis=0))\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "num_samples = len(dataset)\n",
    "num_val_samples = int(val_ratio * num_samples)\n",
    "num_test_samples = int(test_ratio * num_samples)\n",
    "num_train_samples = num_samples - num_val_samples - num_test_samples\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [num_train_samples, num_val_samples, num_test_samples]\n",
    ")\n",
    "\n",
    "# Create DataLoaders for the training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=4, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, prefetch_factor=4, persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, prefetch_factor=4, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_ratio = 0.2\n",
    "# test_ratio = 0.1\n",
    "# batch_size = 128\n",
    "\n",
    "# train_dataset = DroneImagesDataset(csv_file='labeled_images.csv', transform=TRAIN_IMAGE_TRANSFORM)\n",
    "# val_dataset = DroneImagesDataset(csv_file='labeled_images.csv', transform=IMAGE_TRANSFORM)\n",
    "# test_dataset = DroneImagesDataset(csv_file='labeled_images.csv', transform=IMAGE_TRANSFORM)\n",
    "\n",
    "# indices = torch.randperm(len(train_dataset)).tolist()\n",
    "# train_dataset = torch.utils.data.Subset(train_dataset, indices[:-int(len(indices)*(val_ratio+test_ratio))])\n",
    "# val_dataset = torch.utils.data.Subset(val_dataset, indices[-int(len(indices)*(val_ratio+test_ratio)):-int(len(indices)*test_ratio)])\n",
    "# test_dataset = torch.utils.data.Subset(test_dataset, indices[-int(len(indices)*test_ratio):])\n",
    "\n",
    "# # Create DataLoaders for the training, validation, and test sets\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=2, persistent_workers=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, prefetch_factor=2, persistent_workers=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, prefetch_factor=2, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time to check out transforms / see some data blabal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show 3 original images and their corresponding transformed images\n",
    "# fig, axs = plt.subplots(2, 3, figsize=(10, 10))\n",
    "# for i in range(3):\n",
    "#     dataset.transform = None\n",
    "#     # first the original image in colors\n",
    "#     img, label = dataset[i*2]\n",
    "#     axs[0, i].imshow(img.permute(1, 2, 0))\n",
    "#     axs[0, i].set_title(f\"Original Image {i}\")\n",
    "#     # then the transformed image in grayscale\n",
    "#     dataset.transform = IMAGE_TRANSFORM\n",
    "#     img, label = dataset[i*2]\n",
    "#     axs[1, i].imshow(img[0], cmap='gray')\n",
    "#     axs[1, i].set_title(f\"Transformed Image {i}\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning module\n",
    "The config file below saves all your hyperparameters, such that they show up in your weights and biases dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg={\"epochs\": num_epochs,\n",
    "     \"learning_rate\": learning_rate,\n",
    "     \"label_smoothing\": label_smoothing,\n",
    "     \"normalize_images\": normalize_images,\n",
    "     \"architecture\": architecture,\n",
    "     \"comments\": \"no train augment\"\n",
    "     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below actually creats your model, you should not have to change anything here, but don't hesitate to experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_conv_block(architecture, i):\n",
    "    if i == 0:\n",
    "        input_channels = architecture[\"conv_layers\"][\"input_size\"][0]\n",
    "    else:\n",
    "        input_channels = architecture[\"conv_layers\"][\"output_channel\"][i-1]\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=input_channels,\n",
    "                        out_channels=architecture[\"conv_layers\"][\"output_channel\"][i],\n",
    "                        kernel_size=architecture[\"conv_layers\"][\"kernel_size\"][i],\n",
    "                        stride=architecture[\"conv_layers\"][\"stride\"][i],\n",
    "                        padding=architecture[\"conv_layers\"][\"padding\"][i],\n",
    "                        bias= not architecture[\"batch_norm\"]),\n",
    "        nn.BatchNorm2d(architecture[\"conv_layers\"][\"output_channel\"][i]) if architecture[\"batch_norm\"] else nn.Identity(),\n",
    "        nn.MaxPool2d(kernel_size=architecture[\"max_pool_layers\"][\"kernel_size\"][i],\n",
    "                           stride=architecture[\"max_pool_layers\"][\"stride\"][i]),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout2d(architecture[\"dropout_layers\"][\"p\"][i])\n",
    "    )\n",
    "\n",
    "class LightningCNN(L.LightningModule):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [add_conv_block(cfg[\"architecture\"], i) for i in range(cfg[\"architecture\"][\"n_layers\"])] + [nn.Flatten()]\n",
    "\n",
    "        self.embedder = nn.Sequential(*layers)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            training = self.embedder.training\n",
    "            self.embedder.eval()\n",
    "            cfg[\"architecture\"][\"fc_layer\"][\"input_size\"] = self.embedder(torch.empty(1, *cfg[\"architecture\"][\"conv_layers\"][\"input_size\"])).size(-1)\n",
    "            self.embedder.train(training)\n",
    "\n",
    "        self.fc = nn.Linear(cfg[\"architecture\"][\"fc_layer\"][\"input_size\"], cfg[\"architecture\"][\"fc_layer\"][\"output_size\"])\n",
    "\n",
    "        self.save_hyperparameters(cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.embedder(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y, label_smoothing=self.hparams.label_smoothing)\n",
    "        acc = torch.sum(torch.argmax(y_hat, dim=1) == torch.argmax(y, dim=1)) / len(y)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        self.log(\"train/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = torch.sum(torch.argmax(y_hat, dim=1) == torch.argmax(y, dim=1)) / len(y)\n",
    "        self.log(\"val/loss\", loss)\n",
    "        self.log(\"val/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = torch.sum(torch.argmax(y_hat, dim=1) == torch.argmax(y, dim=1)) / len(y)\n",
    "        self.log(\"test/loss\", loss)\n",
    "        self.log(\"test/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=7, gamma=0.5)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wand_blogger = WandbLogger(project=\"MAV-CNN-Project\")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val/acc\", min_delta=0.005, patience=10, verbose=False, mode=\"max\")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = L.Trainer(max_epochs=cfg[\"epochs\"], logger=wand_blogger, callbacks=[early_stop_callback])\n",
    "\n",
    "# Create the model\n",
    "model = LightningCNN(cfg)\n",
    "torchsummary.summary(model, (1, architecture[\"conv_layers\"][\"input_size\"][1], architecture[\"conv_layers\"][\"input_size\"][2]))\n",
    "\n",
    "if False:\n",
    "    # perform inference 10000 times and calculate the average time\n",
    "    import time\n",
    "\n",
    "    model = LightningCNN(cfg)\n",
    "    input = torch.randn(1, 1, 40, 12)\n",
    "\n",
    "    times = []\n",
    "    model.eval()\n",
    "    for i in range(10000):\n",
    "        start = time.time()\n",
    "        model(input)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "\n",
    "    print(f\"Average inference time: {np.mean(times)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to onnx\n",
    "if True:\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 1, architecture[\"conv_layers\"][\"input_size\"][1], architecture[\"conv_layers\"][\"input_size\"][2])\n",
    "    torch.onnx.export(model, dummy_input, f\"models/{wand_blogger.experiment.name}.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
