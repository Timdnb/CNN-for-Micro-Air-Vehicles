{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for CNN training for AE4317 Autonomous Flight of Micro Air Vehicles\n",
    "Author: Tim den Blanken (t.n.a.denblanken@student.tudelft.nl)\n",
    "\n",
    "This notebook assumes you already have a labeled dataset, ready to be used for training. Optionally you can use `Dataset_generation.ipynb` for this, but you could also label them by hand or through any other method. With your own labeled data, you can either tweak the code to match your dataset and labels, or follow the recommended format. This format is as follows: you should have a folder `all_images` containing all your images. Together with this you should have a file `labeled_images.csv` which has the following columns: ['filename', 'your', 'class', 'labels', 'etc'] (any number of classes is possible). The code assumes one-hot encoded labels, if this is not the case, make sure to use the correct loss function for your application.\n",
    "\n",
    "Training your CNN now becomes rather easy, however it is very important to keep track of how your models are performing such that you can compare them. I recommended to use [Weights and Biases](https://wandb.ai/), you can sign up using your GitHub account. Follow the setup procedure on their website to link your API code. When you have done this, it will log all your data such that you can compare performance between models.\n",
    "\n",
    "Make sure to go through the entire notebook at least once, and try to understand what every part does and if it is applicable to your method. In the end you will mostly be using the parameter dashboard, as this where you change the model architecture and hyperparameters.\n",
    "\n",
    "**Keep the following in mind:** \n",
    "\n",
    "Originally this notebook was created to train a CNN to classify each image as 'left', 'forward', 'right', which is then connected to a control algorithm on the drone. If you are following the same approach, you won't have to change much in this notebook. If you are taking a different approach, let's say you classify 'left', 'left-forward', 'forward', 'right-forward', 'right', then you will have to adapt this notebook and the control algorithm on the drone accordingly.\n",
    "\n",
    "Some more interesting points about this notebook:\n",
    "- This notebook uses [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), which is a deep learning framework build on PyTorch, this makes the code more dense and better readable.\n",
    "- At the end of this notebook your model will be saved in the [.onnx](https://onnx.ai/) file format. This allows for the conversion to C code, more about this at the end.\n",
    "\n",
    "Let's dive into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Below are all the imports we need, make sure to have all of them installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "from utils import calc_mean_std_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms  \n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "import torchsummary\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Dashboard\n",
    "This is the main place to tune your CNN. You can fully specify the architecture and a few other hyperparameters, such as the number of epochs and learning rate. All these parameters will also be logged to weights and biases, such that you can see which changes led to better performance. The architecture below is the one used for the competition for AE4317 Autonomous Flight of Micro Air Vehicles of 28-3-24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "label_smoothing = 0.0\n",
    "normalize_images = True\n",
    "batch_norm = True\n",
    "augment_train = True\n",
    "\n",
    "# CNN Architecture\n",
    "architecture = {\n",
    "    \"n_layers\": 4,                              # when changing the number of layers, make sure change the lengths of the tuples below accordingly\n",
    "    \"conv_layers\": {\n",
    "        \"input_size\": (1, 40, 12),              # (input_channgels, height, width)\n",
    "        \"output_channel\": (8, 16, 32, 64),\n",
    "        \"kernel_size\": (3, 3, 3, 3),\n",
    "        \"stride\": (1, 1, 1, 1),\n",
    "        \"padding\": (1, 1, 1, 1),   \n",
    "    },\n",
    "    \"batch_norm\": batch_norm,\n",
    "    \"max_pool_layers\": {\n",
    "        \"kernel_size\": ((2,1), 2, 2, 2),\n",
    "        \"stride\": ((2,1), 2, 2, 2)\n",
    "    },\n",
    "    \"dropout_layers\": {\n",
    "        \"p\": (0.1, 0.05, 0.025, 0.025)\n",
    "    },\n",
    "    \"fc_layer\": {\n",
    "        \"input_size\": None,                     # this will be calculated automatically\n",
    "        \"output_size\": 3                        # change this number if you have more or less than 3 classes\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset and dataloaders\n",
    "Here we create the dataset using the .csv. Make sure it works with your file structure. Also check the distribution of your labels, and correct for possible uneven distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class DroneImagesDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, skiprows=1, header=None)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.annotations.iloc[index, 0]\n",
    "        # img_path = str(Path(img_path)) # TODOOOOO\n",
    "        img_path = img_path.replace(\"\\\\\", \"/\")\n",
    "        image = convert_image_dtype(read_image(img_path), torch.float)\n",
    "        y_label = torch.tensor(list(self.annotations.iloc[index, 1:]), dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, y_label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "# Specify original image size\n",
    "image_size = [520, 240]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we pass the images to the CNN we first want to transform them, such as downscaling. For this we define two transforms, one for training, and one for validation and testing. The transform for training also includes training data augmentation, such as changing the brightness, contrast, saturation, hue and rotation. The goal of this is to make your model more robust to changing conditions. Obviously while testing performance we do not want these augmentation, which is why we have defined two transforms. The current transform converts the image to grayscale, crops the top and bottom 25% and resizes it to the size you specified in the `input_size` in the parameter dashboard. Tweak these transforms to your preferences and try to maximize performance. One important point to keep in mind is that any transform you perform on the data here, you will also have to do on the drone itself. For this reason the interpolation mode is set to NEAREST_EXACT, as this seemed to match the downsampling on the drone the best.\n",
    "\n",
    "Note: the transform used for training data makes the training go rather slow, consider turning off data augmentation while prototyping models and only turn it on once you want to make an already good model more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.3625994920730591, Std: 0.18103595077991486\n"
     ]
    }
   ],
   "source": [
    "IMAGE_TRANSFORM = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.CenterCrop((image_size[0], image_size[1]*0.25)),\n",
    "    transforms.Resize((architecture[\"conv_layers\"][\"input_size\"][1], architecture[\"conv_layers\"][\"input_size\"][2]), interpolation=transforms.InterpolationMode.NEAREST_EXACT),\n",
    "])\n",
    "\n",
    "TRAIN_IMAGE_TRANSFORM = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.25, contrast=0.15, saturation=0.25, hue=0.03),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.CenterCrop((image_size[0], image_size[1]*0.25)),\n",
    "    transforms.Resize((architecture[\"conv_layers\"][\"input_size\"][1], architecture[\"conv_layers\"][\"input_size\"][2]), interpolation=transforms.InterpolationMode.NEAREST_EXACT),\n",
    "])\n",
    "\n",
    "if normalize_images:\n",
    "    # recommenddation: calculate the mean and std of the dataset once, and then hardcode it to save computation time\n",
    "    # mean, std = calc_mean_std_dataset('all_images', IMAGE_TRANSFORM)\n",
    "    mean, std = 0.3625994920730591, 0.18103595077991486\n",
    "    print(f\"Mean: {mean}, Std: {std}\")\n",
    "\n",
    "    IMAGE_TRANSFORM = transforms.Compose([IMAGE_TRANSFORM,\n",
    "        transforms.Normalize(mean=[mean], std=[std])\n",
    "    ])\n",
    "\n",
    "    TRAIN_IMAGE_TRANSFORM = transforms.Compose([TRAIN_IMAGE_TRANSFORM,\n",
    "        transforms.Normalize(mean=[mean], std=[std])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create dataloaders for our training, validation and test set. Feel free to change the sizes for your test and validation set, but don't make them too small!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "batch_size = 128\n",
    "\n",
    "# Set the appropriate transform for the training set\n",
    "if augment_train:\n",
    "    train_transform = TRAIN_IMAGE_TRANSFORM\n",
    "else:\n",
    "    train_transform = IMAGE_TRANSFORM\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset_full = DroneImagesDataset(csv_file='labeled_images.csv', transform=train_transform)\n",
    "val_dataset_full = DroneImagesDataset(csv_file='labeled_images.csv', transform=IMAGE_TRANSFORM)\n",
    "test_dataset_full = DroneImagesDataset(csv_file='labeled_images.csv', transform=IMAGE_TRANSFORM)\n",
    "\n",
    "# Split the datasets\n",
    "indices = torch.randperm(len(train_dataset_full)).tolist()\n",
    "train_dataset = torch.utils.data.Subset(train_dataset_full, indices[:-int(len(indices)*(val_ratio+test_ratio))])\n",
    "val_dataset = torch.utils.data.Subset(val_dataset_full, indices[-int(len(indices)*(val_ratio+test_ratio)):-int(len(indices)*test_ratio)])\n",
    "test_dataset = torch.utils.data.Subset(test_dataset_full, indices[-int(len(indices)*test_ratio):])\n",
    "\n",
    "# Create DataLoaders for the training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=2, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, prefetch_factor=2, persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, prefetch_factor=2, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the codespace below to plot some of your data and check if the transforms are doing what you expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning module\n",
    "The config below saves all your hyperparameters, such that they show up in your weights and biases dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg={\"epochs\": num_epochs,\n",
    "     \"learning_rate\": learning_rate,\n",
    "     \"label_smoothing\": label_smoothing,\n",
    "     \"normalize_images\": normalize_images,\n",
    "     \"architecture\": architecture,\n",
    "     \"augment_train\": augment_train,\n",
    "     \"comments\": \"...\"\n",
    "     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the code that actually creates your model. Make sure to understand the structure and reflect whether the structure makes sense for your data, e.g. if your labels are not one-hot encoded you should use a different loss function. The PyTorch Lightning documentation is a great place to start and understand how all the code works.\n",
    "\n",
    "Note: currently a learning rate scheduler is implemented, which multiplies the learning rate by 0.5 every 7 epochs. You might want to remove or change this based on your approach, if you are unsure how to change it, the PyTorch Lightning documentation explains how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_conv_block(architecture, i):\n",
    "    if i == 0:\n",
    "        input_channels = architecture[\"conv_layers\"][\"input_size\"][0]\n",
    "    else:\n",
    "        input_channels = architecture[\"conv_layers\"][\"output_channel\"][i-1]\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=input_channels,\n",
    "                        out_channels=architecture[\"conv_layers\"][\"output_channel\"][i],\n",
    "                        kernel_size=architecture[\"conv_layers\"][\"kernel_size\"][i],\n",
    "                        stride=architecture[\"conv_layers\"][\"stride\"][i],\n",
    "                        padding=architecture[\"conv_layers\"][\"padding\"][i],\n",
    "                        bias= not architecture[\"batch_norm\"]),\n",
    "        nn.BatchNorm2d(architecture[\"conv_layers\"][\"output_channel\"][i]) if architecture[\"batch_norm\"] else nn.Identity(),\n",
    "        nn.MaxPool2d(kernel_size=architecture[\"max_pool_layers\"][\"kernel_size\"][i],\n",
    "                           stride=architecture[\"max_pool_layers\"][\"stride\"][i]),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout2d(architecture[\"dropout_layers\"][\"p\"][i])\n",
    "    )\n",
    "\n",
    "class LightningCNN(L.LightningModule):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [add_conv_block(cfg[\"architecture\"], i) for i in range(cfg[\"architecture\"][\"n_layers\"])] + [nn.Flatten()]\n",
    "\n",
    "        self.embedder = nn.Sequential(*layers)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            training = self.embedder.training\n",
    "            self.embedder.eval()\n",
    "            cfg[\"architecture\"][\"fc_layer\"][\"input_size\"] = self.embedder(torch.empty(1, *cfg[\"architecture\"][\"conv_layers\"][\"input_size\"])).size(-1)\n",
    "            self.embedder.train(training)\n",
    "\n",
    "        self.fc = nn.Linear(cfg[\"architecture\"][\"fc_layer\"][\"input_size\"], cfg[\"architecture\"][\"fc_layer\"][\"output_size\"])\n",
    "\n",
    "        self.save_hyperparameters(cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.embedder(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y, label_smoothing=self.hparams.label_smoothing)\n",
    "        acc = torch.sum(torch.argmax(y_hat, dim=1) == torch.argmax(y, dim=1)) / len(y)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        self.log(\"train/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = torch.sum(torch.argmax(y_hat, dim=1) == torch.argmax(y, dim=1)) / len(y)\n",
    "        self.log(\"val/loss\", loss)\n",
    "        self.log(\"val/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = torch.sum(torch.argmax(y_hat, dim=1) == torch.argmax(y, dim=1)) / len(y)\n",
    "        self.log(\"test/loss\", loss)\n",
    "        self.log(\"test/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=7, gamma=0.5)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create the logger, and callbacks like an early stopper and a checkpoint saver. Many more callbacks exist, once againg check out the PyTorch Documenation. Aftewards we create the trainer, the model and display a summary of the model. This is a good place to check if everything is still working as intended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wand_blogger = WandbLogger(project=\"MAV-CNN-Project\")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val/acc\", min_delta=0.005, patience=10, verbose=False, mode=\"max\")\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val/acc\", mode=\"max\", filename=\"model-{epoch:02d}\", dirpath=\"checkpoints\")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = L.Trainer(max_epochs=cfg[\"epochs\"], logger=wand_blogger, callbacks=[early_stop_callback, checkpoint_callback])\n",
    "\n",
    "# Create the model\n",
    "model = LightningCNN(cfg)\n",
    "torchsummary.summary(model, (1, architecture[\"conv_layers\"][\"input_size\"][1], architecture[\"conv_layers\"][\"input_size\"][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "With PyTorch Lightning, training is very easy. Just run the line below and you are good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Testing is just as easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to .onnx\n",
    "Finally we save the model to a .onnx file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to onnx\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, 1, architecture[\"conv_layers\"][\"input_size\"][1], architecture[\"conv_layers\"][\"input_size\"][2])\n",
    "torch.onnx.export(model, dummy_input, f\"models/{wand_blogger.experiment.name}.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final steps\n",
    "We now have our (hopefull well performing) model, and it is time to use this model on the drone. For this we need to convert it to C code. Luckily this is really easy when we use [onnx2c](https://github.com/kraiskil/onnx2c). Get their sources, run a standard CMake build and you are ready to go. Read their documentation to see which steps you need to take specifically. After this step you should have your CNN model in C code. **Important:** onnx2c support many types of PyTorch modules, however not all. If you do not touch the layer types in the model architecture, then this is no issue for you. If you do experiment, make sure to check if onnx2c supports it (by simply trying to convert your model to C code).\n",
    "\n",
    "The final step is to integrate your model to run on the drone. You can either give this a shot yourself, or use the branch which contains the cnn modules for paparazzi. When choosing to do the latter, adhere to the following steps:\n",
    "- First make sure you are on the branch that contains the cnn code for the drone.\n",
    "- Then copy all the code from your `model.c` file to the bottom of the `cnn.c` file in `paparazzi/sw/airborne/modules/computer_vision/`, replacing the model that is already there.\n",
    "- Next go to `cnn.h` and define the TENSOR_HEIGHT and TENSOR_WIDTH (input image dimensions) and dataset MEAN and STD in the `cnn.h` file. If you are not using normalized images, put MEAN to 0 and STD to 1.\n",
    "- Finally update the `entry` function declaration to match the image size that you just specified in the step above.\n",
    "\n",
    "If you have also labeled your images 'left', 'forward', 'right' (in that order too), then you should be able to use `cnn_guided` in `paparazzi/sw/airborne/modules/cnn_guided/`, without any necessary changes. If your labeling is different, you will have to write your own or adapt the current control algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional comments\n",
    "- Final model used for competition had test accuracy of ... and test loss of ..., reaching a distance of ... m.\n",
    "- Many improvements are still possible and you are encouraged to not just use this framework, but improve and optimize it\n",
    "- For questions, comments or improvements, don't hesitate to reach out to me"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
