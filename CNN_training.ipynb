{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN training\n",
    "#### Developed during AE4317 Autonomous Flight of Micro Air Vehicles\n",
    "Author: Tim den Blanken (t.n.a.denblanken@student.tudelft.nl)\n",
    "\n",
    "This notebook assumes you already have a labeled dataset, ready to be used for training. Optionally you can use `Dataset_generation.ipynb` for this, but you could also label them by hand or through any other method. With your own labeled data, you can either tweak the code to match your dataset and labels, or follow the recommended format. This format is as follows: you should have a folder `images/all` containing all your images. Together with this you should have a file `labeled_images.csv` which has the following columns: ['filename', 'your', 'class', 'labels', 'etc'] (any number of classes is possible). The code assumes one-hot encoded labels, if this is not the case, make sure to use the correct loss function for your application.\n",
    "\n",
    "Training your CNN now becomes rather easy, however it is very important to keep track of how your models are performing such that you can compare them. I recommended to use [Weights and Biases](https://wandb.ai/), you can sign up using your GitHub account. Follow the setup procedure on their website to link your API code. When you have done this, it will log all your data such that you can compare performance between models.\n",
    "\n",
    "Make sure to go through the entire notebook at least once, and try to understand what every part does and if it is applicable to your method. In the end you will mostly be using the parameter dashboard, as this where you change the model architecture and hyperparameters.\n",
    "\n",
    "**Keep the following in mind:** \n",
    "\n",
    "Originally this notebook was created to train a CNN to classify each image as 'left', 'forward', 'right', which is then connected to a control algorithm on the drone. If you are following the same approach, you won't have to change much in this notebook. If you are taking a different approach, let's say you classify 'left', 'left-forward', 'forward', 'right-forward', 'right', then this notebook will still run fine, however the written control algorithm in C will not work anymore. You will have to adapt that yourself.\n",
    "\n",
    "Some more interesting points about this notebook:\n",
    "- This notebook uses [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), which is a deep learning framework build on PyTorch, this makes the code more dense and better readable.\n",
    "- At the end of this notebook your model will be saved in the [.onnx](https://onnx.ai/) file format. This allows for the conversion to C code, more about this at the end.\n",
    "\n",
    "Let's dive into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Below are all the imports we need, make sure to have all of them installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calc_mean_std_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms  \n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "import torchsummary\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Dashboard\n",
    "This is the main place to tune your CNN. You can fully specify the architecture and a few other hyperparameters, such as the number of epochs and learning rate. All these parameters will also be logged to Weights and Biases, such that you can see which changes led to better performance. The architecture below is the one used for the competition for AE4317 Autonomous Flight of Micro Air Vehicles on the 28th of March, 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN configuration\n",
    "config = {\n",
    "    \"epochs\": 100,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"label_smoothing\": 0.0,\n",
    "    \"normalize_images\": True,\n",
    "    \"augment_train\": True,\n",
    "    \"batch_norm\": True,\n",
    "    \"n_layers\": 4,                              # when changing the number of layers, make sure change the lengths of the tuples below accordingly\n",
    "    \"conv_layers\": {\n",
    "        \"input_size\": (1, 40, 12),              # (input_channgels, height, width)\n",
    "        \"output_channel\": (8, 16, 32, 64),\n",
    "        \"kernel_size\": (3, 3, 3, 3),\n",
    "        \"stride\": (1, 1, 1, 1),\n",
    "        \"padding\": (1, 1, 1, 1),   \n",
    "    },\n",
    "    \"max_pool_layers\": {\n",
    "        \"kernel_size\": ((2,1), 2, 2, 2),\n",
    "        \"stride\": ((2,1), 2, 2, 2)\n",
    "    },\n",
    "    \"dropout_layers\": {\n",
    "        \"p\": (0.1, 0.05, 0.025, 0.025)\n",
    "    },\n",
    "    \"fc_layer\": {\n",
    "        \"input_size\": None,                     # this will be calculated automatically, based on the architecture\n",
    "        \"output_size\": None,                    # this will be calculated automatically, based on the number of classes\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset and dataloaders\n",
    "Here we create the dataset using the .csv. Make sure it works with your file structure. Also check the distribution of your labels, and correct for possible uneven distributions, do this by setting `check_label_distribution` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class DroneImagesDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file, skiprows=1, header=None)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.annotations.iloc[index, 0]\n",
    "        img_path = img_path.replace(\"\\\\\", os.sep)\n",
    "        image = convert_image_dtype(read_image(img_path), torch.float)\n",
    "        y_label = torch.tensor(list(self.annotations.iloc[index, 1:]), dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, y_label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "# First we create a dataset without transform to get the image size and number of classes\n",
    "dataset_no_transform = DroneImagesDataset(\"images/labeled_images.csv\")\n",
    "image_size = dataset_no_transform[0][0].shape\n",
    "config[\"fc_layer\"][\"output_size\"] = len(dataset_no_transform[0][1])\n",
    "\n",
    "# Let's check the label distribution\n",
    "check_label_distribution = False\n",
    "if check_label_distribution:\n",
    "    labels = []\n",
    "    for i in range(len(dataset_no_transform)):\n",
    "        labels.append(dataset_no_transform[i][1].tolist())\n",
    "    labels = np.array(labels)\n",
    "    print(\"Label distribution:\")\n",
    "    print(np.sum(labels, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we pass the images to the CNN we first want to transform them, such as downscaling. For this we define two transforms, one for training, and one for validation and testing. We use a separate transform for training data, since we want to augment the training data to make the model more robust, but we don't want to do these augmentation while validating or testing.\n",
    "\n",
    "The current transform converts the image to grayscale, crops the top and bottom 25% (you can change this through `image_crop`) and resizes it to the size you specified in the `input_size` in the parameter dashboard. Tweak these transforms to your preferences and try to maximize performance. Using bigger images, i.e. resizing it less, will likely improve performance, but will also lead to many more computations. **Important:** any transform you perform on the data here, you will also have to do on the drone itself. For this reason the interpolation mode is set to NEAREST_EXACT, as this seemed to match the downsampling on the drone the best.\n",
    "\n",
    "Note: the transform used for training data makes the training go rather slow, consider turning off data augmentation while prototyping models and only turn it on once you want to make an already good model more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_crop = 0.25\n",
    "\n",
    "IMAGE_TRANSFORM = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.CenterCrop((image_size[1], int(image_size[2]*2*image_crop))),\n",
    "    transforms.Resize((config[\"conv_layers\"][\"input_size\"][1], config[\"conv_layers\"][\"input_size\"][2]), interpolation=transforms.InterpolationMode.NEAREST_EXACT),\n",
    "])\n",
    "\n",
    "TRAIN_IMAGE_TRANSFORM = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.25, contrast=0.15, saturation=0.25, hue=0.03),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.CenterCrop((image_size[1], int(image_size[2]*2*image_crop))),\n",
    "    transforms.Resize((config[\"conv_layers\"][\"input_size\"][1], config[\"conv_layers\"][\"input_size\"][2]), interpolation=transforms.InterpolationMode.NEAREST_EXACT),\n",
    "])\n",
    "\n",
    "if config[\"normalize_images\"]:\n",
    "    # recommendation: calculate the mean and std of the dataset once, and then hardcode it to save computation time\n",
    "    # mean, std = calc_mean_std_dataset('all_images', IMAGE_TRANSFORM)\n",
    "    mean, std = 0.3625994920730591, 0.18103595077991486\n",
    "    print(f\"Mean: {mean}, Std: {std}\")\n",
    "\n",
    "    IMAGE_TRANSFORM = transforms.Compose([IMAGE_TRANSFORM,\n",
    "        transforms.Normalize(mean=[mean], std=[std])\n",
    "    ])\n",
    "\n",
    "    TRAIN_IMAGE_TRANSFORM = transforms.Compose([TRAIN_IMAGE_TRANSFORM,\n",
    "        transforms.Normalize(mean=[mean], std=[std])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create dataloaders for our training, validation and test set. Feel free to change the sizes for your test and validation set, but don't make them too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "batch_size = 128\n",
    "\n",
    "# Set the appropriate transform for the training set\n",
    "if config[\"augment_train\"]:\n",
    "    train_transform = TRAIN_IMAGE_TRANSFORM\n",
    "else:\n",
    "    train_transform = IMAGE_TRANSFORM\n",
    "\n",
    "# Create the datasets\n",
    "train_dataset_full = DroneImagesDataset(csv_file='images/labeled_images.csv', transform=train_transform)\n",
    "val_dataset_full = DroneImagesDataset(csv_file='images/labeled_images.csv', transform=IMAGE_TRANSFORM)\n",
    "test_dataset_full = DroneImagesDataset(csv_file='images/labeled_images.csv', transform=IMAGE_TRANSFORM)\n",
    "\n",
    "# Split the datasets\n",
    "indices = torch.randperm(len(train_dataset_full)).tolist()\n",
    "train_dataset = torch.utils.data.Subset(train_dataset_full, indices[:-int(len(indices)*(val_ratio+test_ratio))])\n",
    "val_dataset = torch.utils.data.Subset(val_dataset_full, indices[-int(len(indices)*(val_ratio+test_ratio)):-int(len(indices)*test_ratio)])\n",
    "test_dataset = torch.utils.data.Subset(test_dataset_full, indices[-int(len(indices)*test_ratio):])\n",
    "\n",
    "# Create DataLoaders for the training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=2, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, prefetch_factor=2, persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, prefetch_factor=2, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some data and see the effect of our transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_original, _ = dataset_no_transform[2]\n",
    "img_transformed, _ = train_dataset_full[2]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(img_original.permute(1, 2, 0))\n",
    "axs[0].set_title('Original image')\n",
    "axs[1].imshow(img_transformed.squeeze(), cmap='gray')\n",
    "axs[1].set_title('Transformed image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN (Lightning) module\n",
    "Next up is the code that actually creates your model. Make sure to understand the structure and reflect whether the structure makes sense for your data, e.g. if your labels are not one-hot encoded you should use a different loss function. The PyTorch Lightning documentation is a great place to start and understand how all the code works.\n",
    "\n",
    "Note: currently a learning rate scheduler is implemented, which multiplies the learning rate by 0.5 every 7 epochs. You might want to remove or change this based on your approach, if you are unsure how to change it, the PyTorch Lightning documentation explains how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_conv_block(config, i):\n",
    "    if i == 0:\n",
    "        input_channels = config[\"conv_layers\"][\"input_size\"][0]\n",
    "    else:\n",
    "        input_channels = config[\"conv_layers\"][\"output_channel\"][i-1]\n",
    "\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels=input_channels,\n",
    "                        out_channels=config[\"conv_layers\"][\"output_channel\"][i],\n",
    "                        kernel_size=config[\"conv_layers\"][\"kernel_size\"][i],\n",
    "                        stride=config[\"conv_layers\"][\"stride\"][i],\n",
    "                        padding=config[\"conv_layers\"][\"padding\"][i],\n",
    "                        bias= not config[\"batch_norm\"]),\n",
    "        nn.BatchNorm2d(config[\"conv_layers\"][\"output_channel\"][i]) if config[\"batch_norm\"] else nn.Identity(),\n",
    "        nn.MaxPool2d(kernel_size=config[\"max_pool_layers\"][\"kernel_size\"][i],\n",
    "                           stride=config[\"max_pool_layers\"][\"stride\"][i]),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout2d(config[\"dropout_layers\"][\"p\"][i])\n",
    "    )\n",
    "\n",
    "class LightningCNN(L.LightningModule):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [add_conv_block(config, i) for i in range(config[\"n_layers\"])] + [nn.Flatten()]\n",
    "\n",
    "        self.embedder = nn.Sequential(*layers)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            training = self.embedder.training\n",
    "            self.embedder.eval()\n",
    "            cfg[\"fc_layer\"][\"input_size\"] = self.embedder(torch.empty(1, *cfg[\"conv_layers\"][\"input_size\"])).size(-1)\n",
    "            self.embedder.train(training)\n",
    "\n",
    "        self.fc = nn.Linear(cfg[\"fc_layer\"][\"input_size\"], cfg[\"fc_layer\"][\"output_size\"])\n",
    "\n",
    "        self.save_hyperparameters(cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.embedder(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y, label_smoothing=self.hparams.label_smoothing)\n",
    "        acc = torch.sum(torch.argmax(y_hat, dim=1) == torch.argmax(y, dim=1)) / len(y)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        self.log(\"train/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = torch.sum(torch.argmax(y_hat, dim=1) == torch.argmax(y, dim=1)) / len(y)\n",
    "        self.log(\"val/loss\", loss)\n",
    "        self.log(\"val/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = torch.sum(torch.argmax(y_hat, dim=1) == torch.argmax(y, dim=1)) / len(y)\n",
    "        self.log(\"test/loss\", loss)\n",
    "        self.log(\"test/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=7, gamma=0.5)\n",
    "        return {\"optimizer\": opt, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the logger to save the model performance and we create callbacks to implement early stopping and to save a model checkpoint. Afterwards we create the trainer, the model and display a summary of the model. This is a good place to check if everything is still working as intended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wand_blogger = WandbLogger(project=\"MAV-CNN-Project\")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val/loss\", min_delta=0.005, patience=10, verbose=False)\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val/loss\", filename=\"model-{epoch:02d}\", dirpath=\"checkpoints\")\n",
    "\n",
    "# Create the trainer\n",
    "trainer = L.Trainer(max_epochs=config[\"epochs\"], logger=wand_blogger, callbacks=[early_stop_callback, checkpoint_callback])\n",
    "\n",
    "# Create the model\n",
    "model = LightningCNN(config)\n",
    "torchsummary.summary(model, config[\"conv_layers\"][\"input_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "With PyTorch Lightning, training is very easy. Just run the line below and you are good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Testing is just as easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to .onnx\n",
    "Finally we save the model to a .onnx file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to onnx\n",
    "model.eval()\n",
    "dummy_input = torch.randn(1, *config[\"conv_layers\"][\"input_size\"])\n",
    "torch.onnx.export(model, dummy_input, f\"models/{wand_blogger.experiment.name}.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final steps\n",
    "We now have our (hopefull well performing) model, and it is time to use this model on the drone. For this we need to convert it to C code. Luckily this is really easy when we use [onnx2c](https://github.com/kraiskil/onnx2c). Get their sources, run a standard CMake build and you are ready to go. Read their documentation to see which steps you need to take specifically. After this step you should have your CNN model in C code. **Important:** onnx2c supports many types of PyTorch modules, however not all. If you do not touch the layer types in the model architecture, then this is no issue for you. If you do experiment, make sure to check if onnx2c supports it (by simply trying to convert your model to C code).\n",
    "\n",
    "The final step is to integrate your model to run on the drone. You can either give this a shot yourself, or use the branch which contains the CNN modules for paparazzi. When choosing to do the latter, adhere to the following steps:\n",
    "1. First make sure you are on the branch that contains the CNN code.\n",
    "2. Then copy all the code from your `model.c` file to the bottom of the `cnn.c` file in `paparazzi/sw/airborne/modules/computer_vision/`, replacing the model that is already there.\n",
    "3. Next go to `cnn.h` and define the `TENSOR_HEIGHT` and `TENSOR_WIDTH` (input image dimensions), the `OUTPUT_CLASSES` (number of classes) and dataset `MEAN` and `STD` in the `cnn.h` file. If you are not using normalized images, put MEAN to 0 and STD to 1.\n",
    "4. Finally update the `entry` function declaration to match the image size that you just specified in the step above.\n",
    "5. If you have also labeled your images 'left', 'forward', 'right' (in that order too), then you can use `cnn_guided` in `paparazzi/sw/airborne/modules/cnn_guided/`, without any necessary changes. If your labeling is different, you will have to write your own or adapt the current control algorithm.\n",
    "6. That's it!\n",
    "\n",
    "Be aware that big CNNs are quite computationally heavy and might not run fast enough on the drone. You should test this speed on the actual drone and not in simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional comments\n",
    "The final model used in the competition had a test accuracy of 0.7986 and test loss of 0.4931, and reached a distance of 131.81 m. It managed to avoid almost all obstacles, including the mystery obstacle. However due to some, still unknown, issue the drone flipped upside down every so many minutes. This is likely due to CPU overload, so keep this in mind when increasing your model size.\n",
    "\n",
    "Lastly, many improvements are still possible to make the CNNs more robust. You are encouraged to not just use this framework to also run CNNs on the drone, but improve it and try to beat our scores! If you have any questions, comments or good ideas for improvement, don't hesitate to reach out to me!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
