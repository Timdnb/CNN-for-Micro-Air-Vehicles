{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Possible -> normalize dataset (subtract mean and divide by std)\n",
    "- add batch / group norm to CNN\n",
    "- learning rate scheduler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "# import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_layers = 3                # not implemented yet\n",
    "# input_size = (1, 130, 60)      # not implemented yet\n",
    "# output_channel = (20, 5, 8)\n",
    "# kernel_size = (7, 5, 3)\n",
    "# padding = (3, 2, 5)\n",
    "# stride = (2, 3, 4)\n",
    "# outputs = 3\n",
    "\n",
    "# def fc_neurons(n_layers, input_size, output_channel, kernel_size, padding, stride):\n",
    "#      for i in range(n_layers):\n",
    "#           print(input_size, i)\n",
    "#           input_size = [output_channel[i], int((input_size[1]+2*padding[i]-(kernel_size[i]-1))/stride[i]), int((input_size[2]+2*padding[i]-(kernel_size[i]-1))/stride[i])]\n",
    "#           print(input_size)\n",
    "#           input_size = [output_channel[i], int(input_size[1]/2), int(input_size[2]/2)]\n",
    "#           print(input_size)\n",
    "#      neurons = input_size[0] * input_size[1] * input_size[2]\n",
    "#      return neurons\n",
    "\n",
    "# print(fc_neurons(n_layers, input_size, output_channel, kernel_size, padding, stride)) # function doesnt work "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Architecture\n",
    "n_layers = 3                # not implemented yet\n",
    "input_size = (1, 26, 15)      # not implemented yet\n",
    "output_channel = (64, 64, 64)\n",
    "kernel_size = (3, 3, 3)\n",
    "padding = (1, 1, 1)\n",
    "stride = (1, 1, 1)\n",
    "dropout = (0.025, 0.025, 0.025)\n",
    "outputs = 3\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20\n",
    "label_smoothing = 0.0\n",
    "learning_rate = 0.0015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_neurons(n_layers, input_size, output_channel, kernel_size, padding, stride):\n",
    "     for i in range(n_layers):\n",
    "          input_size = [output_channel[i], int((input_size[1]+2*padding[i]-(kernel_size[i]-1))/stride[i]), int((input_size[2]+2*padding[i]-(kernel_size[i]-1))/stride[i])]\n",
    "          input_size = [output_channel[i], int(input_size[1]/2), int(input_size[2]/2)]\n",
    "     neurons = input_size[0] * input_size[1] * input_size[2]\n",
    "     return neurons\n",
    "\n",
    "print(fc_neurons(n_layers, input_size, output_channel, kernel_size, padding, stride)) # function doesnt work \n",
    "\n",
    "cfg={\"architecture\": \"CNN\",\n",
    "     \"epochs\": num_epochs,\n",
    "     \"learning_rate\": learning_rate,\n",
    "     \"label_smoothing\": label_smoothing,\n",
    "     \"input_size\": input_size,\n",
    "     \"output_channels\": output_channel,\n",
    "     \"kernel_sizes\": kernel_size,\n",
    "     \"padding\": padding,\n",
    "     \"stride\": stride,\n",
    "     \"dropout\": dropout,\n",
    "     \"fc_layer\": (48, outputs)\n",
    "     }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class DroneImagesDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.annotations = np.genfromtxt(csv_file, delimiter=',', dtype=None, encoding=None, skip_header=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.annotations[index][0]\n",
    "        img_path = str(Path(img_path))\n",
    "        image = convert_image_dtype(read_image(img_path), torch.float)\n",
    "        left, forward, right = float(self.annotations[index][1]), float(self.annotations[index][2]), float(self.annotations[index][3])\n",
    "        y_label = torch.tensor([left, forward, right])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, y_label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to try: bilinear, bicubic or nearest exact\n",
    "IMAGE_TRANSFORM = transforms.Compose([\n",
    "    transforms.CenterCrop((520, 120)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((26, 12), interpolation=transforms.InterpolationMode.NEAREST_EXACT),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "batch_size = 128\n",
    "dataset = DroneImagesDataset(csv_file='labeled_images.csv', transform=IMAGE_TRANSFORM)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "num_samples = len(dataset)\n",
    "num_val_samples = int(val_ratio * num_samples)\n",
    "num_test_samples = int(test_ratio * num_samples)\n",
    "num_train_samples = num_samples - num_val_samples - num_test_samples\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [num_train_samples, num_val_samples, num_test_samples]\n",
    ")\n",
    "\n",
    "# Create DataLoaders for the training, validation, and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=2, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, prefetch_factor=2, persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, prefetch_factor=2, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# # show first 5 grayscale image\n",
    "# for i, (image, y_label) in enumerate(train_loader):\n",
    "#     if i < 5:\n",
    "#         plt.imshow(image[0][0], cmap='gray')\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "for i, image in enumerate(dataset):\n",
    "    # dtore transform\n",
    "    plt.imshow(image[0][0], cmap='gray')\n",
    "    transform = dataset.transform\n",
    "    dataset.transform = None\n",
    "    #plt.imshow(dataset[i][0][0], cmap='gray')\n",
    "    dataset.transform = transform\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningCNN(L.LightningModule):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(cfg)\n",
    "        self.model = torch.nn.Sequential(\n",
    "            # Convolutional layer 1\n",
    "            torch.nn.Conv2d(self.hparams.input_size[0], self.hparams.output_channels[0], kernel_size=self.hparams.kernel_sizes[0], stride=self.hparams.stride[0], padding=self.hparams.padding[0]),\n",
    "            torch.nn.BatchNorm2d(self.hparams.output_channels[0]),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.025),\n",
    "            # Convolutional layer 2\n",
    "            torch.nn.Conv2d(self.hparams.output_channels[0], self.hparams.output_channels[1], kernel_size=self.hparams.kernel_sizes[1], stride=self.hparams.stride[1], padding=self.hparams.padding[1]),\n",
    "            torch.nn.BatchNorm2d(self.hparams.output_channels[1]),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.025),\n",
    "            # Convolutional layer 3\n",
    "            torch.nn.Conv2d(self.hparams.output_channels[1], self.hparams.output_channels[1], kernel_size=self.hparams.kernel_sizes[2], stride=self.hparams.stride[2], padding=self.hparams.padding[2]),\n",
    "            torch.nn.BatchNorm2d(self.hparams.output_channels[1]),\n",
    "            torch.nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.025),\n",
    "            # Fully connected layer\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(self.hparams.fc_layer[0], self.hparams.fc_layer[1]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat, y, label_smoothing=self.hparams.label_smoothing)\n",
    "        loss.backward(retain_graph=True)\n",
    "        acc = torch.sum(torch.argmax(y_hat, dim=1) == torch.argmax(y, dim=1)) / len(y)\n",
    "        self.log(\"train/loss\", loss)\n",
    "        self.log(\"train/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "        acc = torch.sum(torch.argmax(y_hat, dim=1) == torch.argmax(y, dim=1)) / len(y)\n",
    "        self.log(\"val/loss\", loss)\n",
    "        self.log(\"val/acc\", acc)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
    "        acc = torch.sum(torch.argmax(y_hat, dim=1) == torch.argmax(y, dim=1)) / len(y)\n",
    "        self.log(\"test/loss\", loss)\n",
    "        self.log(\"test/acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wand_blogger = WandbLogger(project=\"MAV-CNN-Project\")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val/acc\", min_delta=0.005, patience=10, verbose=False, mode=\"max\")\n",
    "trainer = L.Trainer(max_epochs=cfg[\"epochs\"], logger=wand_blogger, default_root_dir=f\"lightning_logs/{model_name}\", callbacks=[early_stop_callback])\n",
    "model = LightningCNN(cfg)\n",
    "wand_blogger.experiment.name\n",
    "assert(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to onnx\n",
    "# get wandb run name\n",
    "wand_blogger.experiment.name\n",
    "model_name = f\"CNN-e{num_epochs}-\".replace(\".\", \"_\")\n",
    "if True:\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 1, 130, 60)\n",
    "    dummy_input = torch.randn(1, 1, 26, 15)\n",
    "    torch.onnx.export(model, dummy_input, f\"{model_name}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = 'tmp/70251465.jpg'\n",
    "img_2 = 'tmp/84918044.jpg'\n",
    "\n",
    "img_1 = IMAGE_TRANSFORM(convert_image_dtype(read_image(img_1), torch.float))\n",
    "img_2 = IMAGE_TRANSFORM(convert_image_dtype(read_image(img_2), torch.float))\n",
    "\n",
    "img_1 = img_1.reshape(1, *img_1.shape)\n",
    "img_2 = img_2.reshape(1, *img_2.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    print(model(img_1))\n",
    "    print(model(img_2))\n",
    "    model.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
